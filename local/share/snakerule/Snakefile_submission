include: "../snakemake/conf_submission.sk"

# ==========================================================================
#                             EGAsubmitter
# ==========================================================================
# This file is part of EGAsubmitter.
#
# EGAsubmitter is Free Software: you can redistribute it and/or modify it
# under the terms found in the LICENSE.rst file distributed
# together with this file.
#
# EGAsubmitter is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
#
# ==========================================================================
# Author: Marco Viviani <marco.viviani@ircc.it>
# ==========================================================================
#                           Snakefile_submission
# This is the main snakemake pipeline of EGAsubmitter; here there are all
# the processes that create the needed files, link them together where
# needed and submit to the user's EGA account
# ==========================================================================

### --------Preparation part-------- ###
# Following the structure of encrypt/upload we've left a single empty .done file/PHONY here at the end of the pipeline for
# clarity to all users, while all rule chaining internal to the pipeline are now based only on "real" output files.
rule all:
    input: IDS + "/DatasetID"
    output: DONE + "/AllSubmissions.done"
    shell:
        """
            echo "All done:
            Please check the webpage:
            https://ega-archive.org/submitter-portal/#/login
            if everything is fine, you shall proceed with the final submission on your profile."
            touch {output}
        """

### Converts all the yaml files given by the user: the user must place all the files in the yaml/ folder, after filling them out
rule yaml_conversion:
    input: yaml = ancient(USER_METADATA + "/yaml/{what}.yaml")
    output:json = USER_METADATA + "/json/{what}.json"
    run:
        yaml = ruamel.yaml.YAML(typ='safe')
        with open(input.yaml) as yaml_in, open(output.json, "w") as json_out:
            yaml_object = yaml.load(yaml_in)
            json.dump(yaml_object, json_out, indent=1)
### ---- ###
###

## rules to obtain file provisional IDs
rule get_files:
    input: token = ancient("dataset/SessionToken")
    params: path = EGA_URL
    output: USER_METADATA + "/Files_IDs.json"
    shell:
        """
            token=$(cat {input.token})
            curl --request GET {params.path}/files?status=inbox \
                -H "Content-type: application/json" \
                -H "Authorization: Bearer $token" > {output}
        """
rule convert:
    input: json = USER_METADATA + "/Files_IDs.json"
    output: df = USER_METADATA + "/Files_IDs.tsv"
    run:
        pd.read_json(input.json, orient='columns').to_csv(output.df, sep='\t', index=False)

    
### This will build the Runs file, one for each sample: it depends on the encrypt-upload phase last generated file rather than using a subworkflow or other more
### snakemake specific ways of linking snakefiles to be more clear to all users (and to ease remote debugging).
checkpoint buildRuns:
    input:
        encrypted = ancient(TRANSFER + "/logs/done/encrypted-upload.done"),
        IDfiles = USER_METADATA + "/Files_IDs.tsv",
        doneYaml = ancient(expand(USER_METADATA + "/json/{what}.json", what=FILES))
    params:
        tool = SRC_DIR + "/buildRuns.py",
        path = DATASET,
        runType = FILETYPE,
        plate = TEMPLATE
    output:
        csv = SAMPLES_PATH + "/SamplesInformation.csv",
        samples = SAMPLES_PATH + "/Allfiles_list.txt",
        runs = RUNS_PATH + "/Allfiles_list.txt",
        allsamples = USER_METADATA + "/AllSamples_list.txt"
    shell:
        """
            python3 {params.tool} -o {output.csv} -p {params.path} -t {params.runType} -j {params.plate}
        """

rule samples:
    input: csv=SAMPLES_PATH + "/SamplesInformation.csv"
    output: json=SAMPLES_PATH + "/{sample}.json"
    run:
        primary_fields = ["alias", "title", "description", "biological_sex", "subject_id",
                          "phenotype", "bioSample_id", "case_control", "organism_part", "cell_line"]
        sample_data = get_sample_data(input.csv, wildcards.sample, primary_fields)
        
        if sample_data:
            save_json(sample_data, output.json)
        else:
            raise ValueError(f"Sample {wildcards.sample} not found in {input.csv}.")
### --------Preparation part-------- ###

### --------Submission.json part-------- ###

### With this first submission we start the process and get the submission ID from EGA
rule submission:
    input: 
        token = ancient("dataset/SessionToken"),
        json = USER_METADATA + "/json/Submission.json"
    output: id = SUBMISSION_PATH + "/SubmissionID"
    params: 
        path = EGA_URL + "/submissions",
        idbckup = IDBCKUP
    log: SUB_LOGS + "/Submission_info.log"
    shell:
        """
            token=$(cat {input.token})
            curl {params.path} \
                -H "Content-type: application/json" \
                -H "Authorization: Bearer $token" \
                -d @{input.json} > {log}
            if grep -q -E "400|401|500" {log}; then
                echo "Submission of Submission.json failed. The reason could be:"
                cat {log}
                exit 1
            fi
            jq -r '.provisional_id' {log} > {output.id}
            now=$(date +"%d_%m_%Y")
            cp {output.id} {output.id}_$now
            mv {output.id}_$now {params.idbckup}
            echo "Submission ID has been saved in {params.idbckup}"
            echo "Submission.json has been submitted"
        """

### STUDY ###
rule study:
    input: 
        token = ancient("dataset/SessionToken"),
        json = USER_METADATA + "/json/Study.json",
        id = SUBMISSION_PATH + "/SubmissionID"
    output: id = IDS + "/StudyID"
    params: path = EGA_URL
    log: SUB_LOGS + "/Study_submission.log"
    run:
        curl_submit(input.token, input.json, f"{params.path}/submissions/$(cat {input.id})/studies", output.id, log)
### --- ###

### SAMPLES ###
rule samples_submission:
    input:
        token = ancient("dataset/SessionToken"),
        json = SAMPLES_PATH + "/{sample}.json",
        id = SUBMISSION_PATH + "/SubmissionID"
    output: id = SAMPLES_PATH + "/IDs/{sample}_ID"
    params: path = EGA_URL
    log: SUB_LOGS + "/samples/{sample}.log"
    run:
        curl_submit(input.token, input.json, f"{params.path}/submissions/$(cat {input.id})/samples", output.id, log)
### --- ###

### EXPERIMENT ###
### Here the tool recover the Study ID (submitted before) and link it to the experiment
rule experimentAlias:
    input:
        getSample,
        json = ancient(USER_METADATA + "/json/Experiment.json"),
        idStudy = IDS + "/StudyID"
    output: after = SUBMISSION_PATH + "/Experiment.json"
    run:
        with open(input.idStudy) as i:
            study_id = i.readline().strip()
        with open(input.json) as file:
            df = json.load(file)
            df['study_provisional_id'] = study_id
        save_json(df, output.after)
rule experiment_submission:
    input:
        token = ancient("dataset/SessionToken"),
        json = SUBMISSION_PATH + "/Experiment.json",
        id = SUBMISSION_PATH + "/SubmissionID"
    output: id = SUBMISSION_PATH + "/IDs/Experiment_ID"
    params: path = EGA_URL
    log: SUB_LOGS + "/experimentSubmission.log"
    run:
        curl_submit(input.token, input.json, f"{params.path}/submissions/$(cat {input.id})/experiments", output.id, log)
### --- ###

### RUNS ###
### Here the tool recover the Experiment ID and Samples IDs (submitted before) and link them to the Runs;
### Each Sample ID is linked to its specific Run file
rule runsAlias:
    input:
        getSample,
        json = ancient(RUNS_PATH + "/Run_{sample}.json"),
        idExp = SUBMISSION_PATH + "/IDs/Experiment_ID",
        idSample = SAMPLES_PATH + "/IDs/{sample}_ID"
    output: after = SUBMISSION_PATH + "/runs/Run_{sample}.json"
    run:
        with open(input.idExp) as i, open(input.idSample) as s:
            exp_id = i.readline().strip()
            sample_id = s.readline().strip()
        with open(input.json) as file:
            df = json.load(file)
            df['experiment_provisional_id'] = exp_id
            df['sample_provisional_id'] = sample_id
        save_json(df, output.after)
rule runsSubmission:
    input:
        token = ancient('dataset/SessionToken'),
        json = SUBMISSION_PATH + "/runs/Run_{sample}.json",
        id = SUBMISSION_PATH + "/SubmissionID"
    output: id = RUNS_PATH + "/IDs/Run_{sample}_ID"
    params: path = EGA_URL
    log: SUB_LOGS + "/runs/Run_{sample}.log"
    run:
        curl_submit(input.token, input.json, f"{params.path}/submissions/$(cat {input.id})/runs", output.id, log)
### --- ###

### DATASET ###
### Lastly, EGAsubmitter links every run in the Dataset.json
rule datasetAlias:
    input:
        getRun,
        json = ancient(USER_METADATA + "/json/Dataset.json")
    params: runIDpath = ancient(RUNS_PATH + "/IDs/")
    output: after = SUBMISSION_PATH + "/Dataset.json"
    run:
        with open(input.json) as file:
            df = json.load(file)
        for filename in os.listdir(params.runIDpath):
            if filename.endswith("_ID"):
                with open(os.path.join(params.runIDpath, filename), 'r') as f:
                    run_id = f.readline().strip()
                    df['run_provisional_ids'].append(run_id)
        save_json(df, output.after)
rule dataset:
    input:
        token = ancient('dataset/SessionToken'),
        json = SUBMISSION_PATH + "/Dataset.json",
        id = SUBMISSION_PATH + "/SubmissionID"
    output: id = IDS + "/DatasetID"
    params: path = EGA_URL
    log: SUB_LOGS + "/Dataset_submission.log"
    run:
        curl_submit(input.token, input.json, f"{params.path}/submissions/$(cat {input.id})/datasets", output.id, log)

### --- ###
### --------Submission part-------- ###
